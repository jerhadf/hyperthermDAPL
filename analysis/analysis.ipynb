{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Analysis File\n",
    "This file contains the main data analysis code for the project. It is divided into 3 main parts:\n",
    "1. Data Cleaning\n",
    "2. Exploratory Data Analysis\n",
    "3. Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jerhadf/opt/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "# CORE\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np  # Numpy for numerical computations and array operations\n",
    "import pandas as pd  # Pandas for data manipulation and analysis\n",
    "\n",
    "# MACHINE LEARNING & STATISTICS \n",
    "import scipy.stats as stats  # SciPy for scientific computing and technical computing, including statistics\n",
    "import sklearn as sk # Scikit-learn for machine learning and predictive modeling\n",
    "\n",
    "# VISUALIZATION\n",
    "import matplotlib.pyplot as plt  # Matplotlib for creating static, animated, and interactive visualizations\n",
    "import seaborn as sns  # Seaborn for statistical data visualization built on top of Matplotlib\n",
    "import plotly.express as px  # Plotly Express for creating interactive plots and charts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepaths of all of the csv data files to be analyzed\n",
    "\n",
    "# autonest_csv = \"CEIP_csv/AutoNest.csv\"\n",
    "# autonest_strategy_csv = \"CEIP_csv/AutoNestStrategy.csv\"\n",
    "# material_csv = \"CEIP_csv/Material.csv\"\n",
    "# nest_csv = \"CEIP_csv/Nest.csv\"\n",
    "# part_csv = \"CEIP_csv/Part.csv\"\n",
    "# performance_csv = \"CEIP_csv/Performance.csv\"\n",
    "training_csv = \"../CEIP_csv/training_data.csv\"\n",
    "\n",
    "# read in all of these csv files as pandas dataframes\n",
    "\n",
    "# autonest_df = pd.read_csv(autonest_csv)\n",
    "# autonest_strategy_df = pd.read_csv(autonest_strategy_csv)\n",
    "# material_df = pd.read_csv(material_csv)\n",
    "# nest_df = pd.read_csv(nest_csv)\n",
    "# part_df = pd.read_csv(part_csv)\n",
    "# performance_df = pd.read_csv(performance_csv)\n",
    "training_df = pd.read_csv(training_csv) # takes about 1 min to read "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Previewing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ixJobSummary', 'ixNest', 'ixPart', 'dPartTrueArea', 'cRequired', 'cNested', 'ixMaterial', 'fExtShape', 'dExtArea', 'dExtBoundaryDist', 'dExtContainedDist', 'dLgIntArea', 'dLgIntBoundaryDist', 'dLgIntContainedDist', 'dLgExtConArea', 'dLgExtConBoundaryDist', 'dLgExtConContainedDist', 'cTimesCut', 'dNestingTime', 'fStrategies', 'dSheetLength', 'dSheetWidth', 'dSheetArea', 'dLengthUsed', 'dWidthUsed', 'dPartArea', 'calcUtil', 'ixAutoNestStrategy', 'fAllPartsNested']\n"
     ]
    }
   ],
   "source": [
    "# count the number of null values \n",
    "# training_df.isnull().sum() # --> RESULT: no null values in any column  \n",
    "\n",
    "column_list = list(training_df.columns)\n",
    "print(column_list)\n",
    "\n",
    "# count the values for the ixMaterial column in the training_df \n",
    "# training_df.ixMaterial.value_counts()\n",
    "\n",
    "# plot the distribution of the ixMaterial column in the training_df\n",
    "# only plot the 10 most common values\n",
    "# training_df.ixMaterial.value_counts().nlargest(10).plot(kind='bar', figsize=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ixJobSummary               224892\n",
       "ixNest                     224892\n",
       "ixPart                    4200357\n",
       "dPartTrueArea              984974\n",
       "cRequired                    1534\n",
       "cNested                      1654\n",
       "ixMaterial                   7316\n",
       "fExtShape                      52\n",
       "dExtArea                   825580\n",
       "dExtBoundaryDist           159907\n",
       "dExtContainedDist          526206\n",
       "dLgIntArea                  74924\n",
       "dLgIntBoundaryDist          35721\n",
       "dLgIntContainedDist         83215\n",
       "dLgExtConArea               71156\n",
       "dLgExtConBoundaryDist       24334\n",
       "dLgExtConContainedDist      68328\n",
       "cTimesCut                     176\n",
       "dNestingTime                23997\n",
       "fStrategies                   292\n",
       "dSheetLength                 2917\n",
       "dSheetWidth                  1937\n",
       "dSheetArea                  13217\n",
       "dLengthUsed                136950\n",
       "dWidthUsed                 112688\n",
       "dPartArea                  168268\n",
       "calcUtil                   172012\n",
       "ixAutoNestStrategy             13\n",
       "fAllPartsNested                 2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count the number of unique values\n",
    "training_df.nunique(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# above shows that there are only 4.2 million unique values for ixPart \n",
    "# this indicates that there are a lot of rows that are duplicated? \n",
    "\n",
    "# remove duplicates \n",
    "training_df = training_df.drop_duplicates()\n",
    "training_df.shape # --> (5,762,622 rows, 29 columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5762622, 29)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by one or more columns and calculate the mean of a specific column\n",
    "# grouped_data = df.groupby(['column1', 'column2']).mean()\n",
    "# Perform a cross-tabulation between two columns \n",
    "# cross_tab = pd.crosstab(df['column1'], df['column2'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Re-encoding Variables\n",
    "* One-hot encode all variables that are numerical but represent categories\n",
    "* Replace ixMaterial and ixAutoNestStrategy with their appropriate values from the JSON file \n",
    "* Limit to only Materials that are steel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace ixMaterial column with the values from the MaterialTypes.json file \n",
    "# read in the json file as a pandas dataframe\n",
    "material_types_df = pd.read_json('../CEIP_csv/MaterialTypes.json')\n",
    "\n",
    "# create a dictionary of the material types and their corresponding values \n",
    "material_types_dict = material_types_df.to_dict()['ixMaterial']\n",
    "\n",
    "# replace the values in the ixMaterial column with the corresponding material type\n",
    "training_df.ixMaterial = training_df.ixMaterial.replace(material_types_dict)\n",
    "\n",
    "# df_cleaned['column'] = df_cleaned['column'].replace({'old_value': 'new_value'})\n",
    "\n",
    "\n",
    "# # Apply custom functions to clean data (e.g., using .apply() or .map())\n",
    "# def clean_function(value):\n",
    "#     # Your cleaning logic here\n",
    "#     return cleaned_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding for the ixMaterial column\n",
    "training_df = pd.get_dummies(training_df, columns=['ixMaterial'])\n",
    "\n",
    "# One-hot encoding\n",
    "# df_cleaned = pd.get_dummies(df_cleaned, columns=['categorical_column'])\n",
    "\n",
    "# # Label encoding\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# le = LabelEncoder()\n",
    "# df_cleaned['categorical_column'] = le.fit_transform(df_cleaned['categorical_column'])\n",
    "\n",
    "# # Ordinal encoding\n",
    "# from sklearn.preprocessing import OrdinalEncoder\n",
    "# encoder = OrdinalEncoder()\n",
    "# df['ordinal_encoded_feature'] = encoder.fit_transform(df[['categorical_column']]) \n",
    "\n",
    "# # Target encoding\n",
    "# from category_encoders import TargetEncoder\n",
    "# te = TargetEncoder()\n",
    "# df_cleaned['categorical_column'] = te.fit_transform(df_cleaned['categorical_column'], df_cleaned['target_column'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter to only materials that are in the SteelMaterials.json file \n",
    "# read in the json file as a pandas dataframe\n",
    "steel_materials_df = pd.read_json('../CEIP_csv/SteelMaterials.json')\n",
    "\n",
    "# create a list of the steel materials \n",
    "steel_materials_list = list(steel_materials_df.ixMaterial)\n",
    "\n",
    "# filter the training_df to only include steel materials\n",
    "training_df = training_df[training_df.ixMaterial.isin(steel_materials_list)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a numeric variable to categorical using custom ranges\n",
    "# bins = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "# labels = ['0-10', '11-20', '21-30', '31-40', '41-50', '51-60', '61-70', '71-80', '81-90', '91-100']\n",
    "# df['age_group'] = pd.cut(df['age'], bins=bins, labels=lab"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downsampling Data\n",
    "* Downsampling data to ~1,000,000 samples for early machine learning & analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsampling code "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize outliers using a boxplot\n",
    "# sns.boxplot(data=df_cleaned['column'])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate the IQR for a specific column\n",
    "# Q1 = df_cleaned['column'].quantile(0.25)\n",
    "# Q3 = df_cleaned['column'].quantile(0.75)\n",
    "# IQR = Q3 - Q1\n",
    "\n",
    "# # Remove outliers based on the IQR\n",
    "# df_cleaned = df_cleaned[(df_cleaned['column'] >= Q1 - 1.5 * IQR) & (df_cleaned['column'] <= Q3 + 1.5 * IQR)]\n",
    "\n",
    "# # Alternative: Remove rows with values outside a specified range\n",
    "# df = df[(df['variable'] >= lower_bound) & (df['variable'] <= upper_bound)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the column to numeric data type (if not already numeric)\n",
    "# df[var] = pd.to_numeric(df[var], errors='coerce')\n",
    "\n",
    "# # Calculate the IQR and define the threshold values\n",
    "# Q1 = df[var].quantile(0.25)\n",
    "# Q3 = df[var].quantile(0.75)\n",
    "# IQR = Q3 - Q1\n",
    "# threshold_low = Q1 - 1.5 * IQR\n",
    "# threshold_high = Q3 + 1.5 * IQR\n",
    "\n",
    "# Create a new DataFrame with the outlier values removed\n",
    "# df_no_outliers = df[(df[var] >= threshold_low) & (df[var] <= threshold_high)]\n",
    "\n",
    "# Define the outlier detection function using the IQR method\n",
    "# def remove_outliers_iqr(df, column):\n",
    "#     Q1 = df[column].quantile(0.25)\n",
    "#     Q3 = df[column].quantile(0.75)\n",
    "#     IQR = Q3 - Q1\n",
    "#     threshold_low = Q1 - 1.5 * IQR\n",
    "#     threshold_high = Q3 + 1.5 * IQR\n",
    "#     return df[(df[column] >= threshold_low) & (df[column] <= threshold_high)]\n",
    "\n",
    "# # Remove outliers from all columns in the DataFrame\n",
    "# for column in df.columns:\n",
    "#     df = remove_outliers_iqr(df, column)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature creation\n",
    "* Making new features based on Mark's - what can we make? See the CEIP_db/Mark_calculations folder for more details on what he did "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a new feature by combining two existing features\n",
    "# df['new_feature'] = df['column1'] * df['column2']\n",
    "\n",
    "# # Create a new feature by applying a mathematical operation\n",
    "# df['log_feature'] = np.log(df['column1'])\n",
    "\n",
    "# # Create a new feature by applying a custom function\n",
    "# def custom_function(x):\n",
    "#     return x**2\n",
    "\n",
    "# df['squared_feature'] = df['column1'].apply(custom_function)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling & Normalization\n",
    "Bring features to a similar scale to prevent one from dominating the  - some ML models sensitive to feature magnitudes, can perform poorly on different scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# # Standard Scaling (Z-score normalization)\n",
    "# scaler = StandardScaler()\n",
    "# df['standard_scaled_feature'] = scaler.fit_transform(df[['column1']])\n",
    "\n",
    "# # Min-Max Scaling (Normalization)\n",
    "# scaler = MinMaxScaler()\n",
    "# df['min_max_scaled_feature'] = scaler.fit_transform(df[['column1']])\n",
    "\n",
    "# Robust Scaling: Scale features using median and interquartile range, making it less sensitive to outliers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# automated EDA with AutoViz\n",
    "eda()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histograms\n",
    "# plt.hist(df_cleaned['column'], bins=20)\n",
    "# plt.xlabel('Column Name')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.title('Histogram of Column Name')\n",
    "# plt.show()\n",
    "\n",
    "# box plots \n",
    "# sns.boxplot(x=df_cleaned['column'])\n",
    "# plt.xlabel('Column Name')\n",
    "# plt.title('Box Plot of Column Name')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plots \n",
    "# plt.scatter(df_cleaned['column1'], df_cleaned['column2'])\n",
    "# plt.xlabel('Column 1')\n",
    "# plt.ylabel('Column 2')\n",
    "# plt.title('Scatter Plot of Column 1 vs. Column 2')\n",
    "# plt.show()\n",
    "\n",
    "# interactive scatter plot \n",
    "# fig = px.scatter(df_cleaned, x='column1', y='column2', color='category_column', hover_data=['column3'])\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bubble plots - 3 numerical variables\n",
    "# plt.scatter(df_cleaned['column1'], df_cleaned['column2'], s=df_cleaned['column3'], alpha=0.5)\n",
    "# plt.xlabel('Column 1')\n",
    "# plt.ylabel('Column 2')\n",
    "# plt.title('Bubble Plot of Column 1 vs. Column 2 (Size = Column 3)')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallel coordinates plot for multiple numerical variables\n",
    "# from pandas.plotting import parallel_coordinates\n",
    "\n",
    "# parallel_coordinates(df_cleaned[['column1', 'column2', 'column3', 'category_column']], 'category_column')\n",
    "# plt.title('Parallel Coordinates Plot')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regression plotting \n",
    "# sns.regplot(x='column1', y='column2', data=df_cleaned)\n",
    "# plt.title('Regression Plot')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Things to visualize: \n",
    "# tradeoffs between nesting time and utilization\n",
    "# distribution of utilization \n",
    "# relationship between fStrategies and utilization\n",
    "# visualization of the frequency of different Strategies used\n",
    "# visualization of the frequency of different Materials used \n",
    "# distribution of Parts, number of Parts per job, number of Parts nested \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the variable of interest\n",
    "# selected_var = 'dCropUtil'\n",
    "\n",
    "# # Compute the correlation coefficients between the selected variable and all other variables\n",
    "# corr_values = df.corr()[selected_var]\n",
    "\n",
    "# # Compute the correlation coefficients between all pairs of variables\n",
    "# corr_values = df.corr()\n",
    "\n",
    "# # Display the correlation coefficients\n",
    "# print(corr_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Display the correlation matrix as a heatmap for better visualization\n",
    "# sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a histogram of the selected variable\n",
    "# sns.histplot(data=df_no_outliers, x=selected_var, kde=True)\n",
    "# plt.title('Histogram of {}'.format(selected_var))\n",
    "# plt.xlabel(selected_var)\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.show()\n",
    "\n",
    "# # Plot a boxplot of the selected variable\n",
    "# sns.boxplot(data=df_no_outliers, x=selected_var)\n",
    "# plt.title('Boxplot of {}'.format(selected_var))\n",
    "# plt.xlabel(selected_var)\n",
    "# plt.show()\n",
    "\n",
    "# # Compute the summary statistics of the selected variable\n",
    "# summary = df_no_outliers[selected_var].describe()\n",
    "# print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove highly correlated features\n",
    "# corr_matrix = df_cleaned.corr().abs()\n",
    "# upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "# cols_to_drop = [column for column in upper_triangle.columns if any(upper_triangle[column] > correlation_threshold)]\n",
    "# df_cleaned.drop(cols_to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.cluster import KMeans\n",
    "\n",
    "# # Create a K-means clustering model with 3 clusters\n",
    "# kmeans = KMeans(n_clusters=3, random_state=0).fit(df)\n",
    "\n",
    "# # Get the cluster labels for each data point\n",
    "# labels = kmeans.labels_\n",
    "\n",
    "# # Add cluster labels to the original DataFrame\n",
    "# df['Cluster'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with PyCaret\n",
    "# import pycaret clustering and init setup\n",
    "# see https://colab.research.google.com/github/pycaret/pycaret/blob/master/tutorials/Tutorial%20-%20Clustering.ipynb#scrollTo=4181de41\n",
    "from pycaret.clustering import *\n",
    "s = setup(data, session_id = 123)\n",
    "\n",
    "# import ClusteringExperiment and init the class\n",
    "from pycaret.clustering import ClusteringExperiment\n",
    "exp = ClusteringExperiment()\n",
    "\n",
    "# init setup on exp\n",
    "exp.setup(data, session_id = 123)\n",
    "\n",
    "# train kmeans model\n",
    "kmeans = create_model('kmeans')\n",
    "\n",
    "# to check all the available models\n",
    "models()\n",
    "\n",
    "# train meanshift model\n",
    "meanshift = create_model('meanshift')\n",
    "\n",
    "kmeans_cluster = assign_model(kmeans)\n",
    "kmeans_cluster\n",
    "\n",
    "# plot pca cluster plot \n",
    "plot_model(kmeans, plot = 'cluster')\n",
    "\n",
    "# plot elbow\n",
    "plot_model(kmeans, plot = 'elbow')\n",
    "\n",
    "# plot silhouette\n",
    "plot_model(kmeans, plot = 'silhouette')\n",
    "\n",
    "evaluate_model(kmeans)\n",
    "\n",
    "# predict on test set\n",
    "kmeans_pred = predict_model(kmeans, data=data)\n",
    "kmeans_pred"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model exploration with PyCaret\n",
    "\n",
    "Try PyCaret to start \n",
    "https://github.com/pycaret/pycaret/blob/master/tutorials/Tutorial%20-%20Regression.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = training_df\n",
    "\n",
    "# import pycaret regression and init setup\n",
    "from pycaret.regression import *\n",
    "s = setup(data, target = 'charges', session_id = 123)\n",
    "\n",
    "# import RegressionExperiment and init the class\n",
    "from pycaret.regression import RegressionExperiment\n",
    "exp = RegressionExperiment()\n",
    "\n",
    "# init setup on exp\n",
    "exp.setup(data, target = 'charges', session_id = 123)\n",
    "\n",
    "# compare baseline models\n",
    "best = compare_models()\n",
    "\n",
    "# compare models using OOP\n",
    "# exp.compare_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the models and interpret \n",
    "# plot residuals\n",
    "plot_model(best, plot = 'residuals')\n",
    "\n",
    "# plot error\n",
    "plot_model(best, plot = 'error')\n",
    "\n",
    "# plot feature importance\n",
    "plot_model(best, plot = 'feature')\n",
    "\n",
    "evaluate_model(best)\n",
    "\n",
    "# train lightgbm model\n",
    "lightgbm = create_model('lightgbm')\n",
    "\n",
    "# interpret summary model\n",
    "interpret_model(lightgbm, plot = 'summary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on test set\n",
    "holdout_pred = predict_model(best)\n",
    "\n",
    "# show predictions df\n",
    "holdout_pred.head()\n",
    "\n",
    "# copy data and drop charges\n",
    "new_data = data.copy()\n",
    "new_data.drop('charges', axis=1, inplace=True)\n",
    "new_data.head()\n",
    "\n",
    "# predict model on new_data\n",
    "predictions = predict_model(best, data = new_data)\n",
    "predictions.head()\n",
    "\n",
    "\n",
    "# save pipeline\n",
    "save_model(best, 'my_first_pipeline')\n",
    "\n",
    "# load pipeline\n",
    "loaded_best_pipeline = load_model('my_first_pipeline')\n",
    "loaded_best_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensembling \n",
    "\n",
    "# train a dt model with default params\n",
    "dt = create_model('dt')\n",
    "\n",
    "# tune hyperparameters of dt\n",
    "tuned_dt = tune_model(dt)\n",
    "\n",
    "# define tuning grid\n",
    "dt_grid = {'max_depth' : [None, 2, 4, 6, 8, 10, 12]}\n",
    "\n",
    "# tune model with custom grid and metric = MAE\n",
    "tuned_dt = tune_model(dt, custom_grid = dt_grid, optimize = 'MAE')\n",
    "\n",
    "# tune dt using optuna\n",
    "tuned_dt = tune_model(dt, search_library = 'optuna')\n",
    "\n",
    "# ensemble with bagging\n",
    "ensemble_model(dt, method = 'Bagging')\n",
    "\n",
    "# ensemble with boosting\n",
    "ensemble_model(dt, method = 'Boosting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # top 3 models based on mae\n",
    "best_mae_models_top3 = compare_models(n_select = 3, sort = 'MAE')\n",
    "\n",
    "# blending models \n",
    "blend_models(best_mae_models_top3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find best model based on CV metrics\n",
    "# returns the best model out of all trained models in the current setup based on the optimize parameter\n",
    "automl() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dashboard function\n",
    "dashboard(dt, display_format ='inline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finalize the model \n",
    "final_best = finalize_model(best)\n",
    "\n",
    "# save model\n",
    "# save_model(best, 'my_first_model')\n",
    "\n",
    "\n",
    "# load model\n",
    "# loaded_from_disk = load_model('my_first_model')\n",
    "# loaded_from_disk\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic models to try \n",
    "* Linear Regression - Use sci-kit learn to implement\n",
    "* Logistic Regression\n",
    "* Support Vector Machines\n",
    "* Basic decision trees\n",
    "* Naive Bayes\n",
    "\n",
    "### XGBoost \n",
    "Tree ensemble model that can handle tabular, numerical, low-dimensional data very well. Fast and scalable, and can be hyperparameter tuned to optimize performance.\n",
    "\n",
    "### ResNet-like architecture\n",
    "Neural network architecture adapted for tabular data, allowing learning from shallow and deep features. Use PyTorch or TensorFlow to implement. Less interpretable. \n",
    "\n",
    "### Transformer \n",
    "Use Transformers with tabular data? \n",
    "\n",
    "### KNN - K Nearest Neighbors\n",
    "Predicts the target variable based on the similarity of the features with the nearest neighbors in the training dataset. Computationally expensive, but can be used for regression and classification."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "* Cross-validation\n",
    "* Classification Accuracy\n",
    "* Confusion Matrix\n",
    "* ROC-AUC Curve\n",
    "  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Fine-tuning\n",
    "* Hyperparameter tuning\n",
    "* Ensembling multiple models together - voting classifier, bagging, boosting, XGBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Univariate Feature Selection:** This method uses statistical tests like chi-squared tests or ANOVA to evaluate the relationship between each feature and the target variable independently. It ranks the features based on their significance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_selection import SelectKBest, chi2, f_classif\n",
    "\n",
    "# # For categorical target using Chi-squared test\n",
    "# selector = SelectKBest(score_func=chi2, k=5)\n",
    "# selector.fit_transform(X, y)\n",
    "\n",
    "# # For continuous target using ANOVA F-value\n",
    "# selector = SelectKBest(score_func=f_classif, k=5)\n",
    "# selector.fit_transform(X, y)\n",
    "\n",
    "# # Get feature importances\n",
    "# scores = selector.scores_\n",
    "# feature_importances = pd.DataFrame({'feature': X.columns, 'importance': scores})\n",
    "# feature_importances = feature_importances.sort_values('importance', ascending=False)\n",
    "# print(feature_importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other statistical testing \n",
    "# import scipy.stats as stats\n",
    "\n",
    "# # Perform a t-test to compare the means of two groups\n",
    "# t_stat, p_value = stats.ttest_ind(group1, group2)\n",
    "# print(\"T-statistic:\", t_stat, \"P-value:\", p_value)\n",
    "\n",
    "# # Perform a chi-squared test to determine the association between two categorical variables\n",
    "# chi_stat, p_value, dof, ex = stats.chi2_contingency(contingency_table)\n",
    "# print(\"Chi-squared statistic:\", chi_stat, \"P-value:\", p_value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
