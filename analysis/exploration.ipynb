{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis \n",
    "EDA + feature engineering + also some more cleaning (outlier removal mostly)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORE\n",
    "import pandas as pd\n",
    "import numpy as np  # Numpy for numerical computations and array operations\n",
    "import pandas as pd  # Pandas for data manipulation and analysis\n",
    "\n",
    "# MACHINE LEARNING & STATISTICS \n",
    "import scipy.stats as stats  # SciPy for scientific computing and technical computing, including statistics\n",
    "import sklearn as sk # Scikit-learn for machine learning and predictive modeling\n",
    "\n",
    "# VISUALIZATION\n",
    "import matplotlib.pyplot as plt  # Matplotlib for creating static, animated, and interactive visualizations\n",
    "import seaborn as sns  # Seaborn for statistical data visualization built on top of Matplotlib\n",
    "import plotly.express as px  # Plotly Express for creating interactive plots and charts\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the cleaned data that was output by cleaning.ipynb\n",
    "df = pd.read_csv('../CEIP_csv/cleaned2.csv')\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the variable of interest\n",
    "# selected_var = 'dCropUtil'\n",
    "\n",
    "# # Compute the correlation coefficients between the selected variable and all other variables\n",
    "# corr_values = df.corr()[selected_var]\n",
    "\n",
    "# # Compute the correlation coefficients between all pairs of variables\n",
    "# corr_values = df.corr()\n",
    "\n",
    "# # Display the correlation coefficients\n",
    "# print(corr_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove highly correlated features if needed\n",
    "# corr_matrix = df_cleaned.corr().abs()\n",
    "# upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "# cols_to_drop = [column for column in upper_triangle.columns if any(upper_triangle[column] > correlation_threshold)]\n",
    "# df_cleaned.drop(cols_to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list the columns in the dataframe\n",
    "column_list = df.columns.tolist()\n",
    "\n",
    "# list of index columns (like ixNest, ixJobSummary, ixPart)\n",
    "indices = [column for column in column_list if column.startswith('ix')]\n",
    "\n",
    "# list of strategy type column names \n",
    "strategies = [column for column in column_list if column.startswith('_')]\n",
    "\n",
    "nest_data = ['cTimesCut', 'fAllPartsNested', 'cParts', 'fStrategies', 'dNestingTime', \n",
    "             'dLengthUsed', 'dWidthUsed', 'dPartArea']\n",
    "# cTimesCut - removed, # of times nest will be cut in job, not known in advance \n",
    "# fAllPartsNested - removed, just a 0/1 indicator of if all the parts in the job were nested using automatic nesting\n",
    "# cParts - Total number of parts nested from Nest table - duplicated by cNested, unknown prior to nesting \n",
    "# analysis - check if cParts is identical to cNested, what are the differences \n",
    "# fStrategies - what is the meaning of this? does it add anything? \n",
    "# dNestingTime - analyze later but not a feature \n",
    "# dLengthUsed & dWidthUsed - Length/width of plate used by nested parts - not features \n",
    "# dPartArea - Total area of nested parts - replicated by dPartTrueArea, from Nest Table \n",
    "\n",
    "# exclude the # of parts nested and the # required - not part characteristics \n",
    "others = ['cNested', 'cRequired']\n",
    "\n",
    "# set the target \n",
    "target = ['calcUtil']\n",
    "\n",
    "# select all the columns that are actually features \n",
    "non_features = set().union(indices, strategies, nest_data, others, target)\n",
    "\n",
    "# Get all columns from the DataFrame as a set\n",
    "all_columns_set = set(df.columns)\n",
    "\n",
    "# Subtract the sets of columns you don't want from the set of all columns\n",
    "selected_columns_set = all_columns_set - non_features\n",
    "feature_list = list(selected_columns_set) # Convert the resulting set back to a list\n",
    "\n",
    "feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.cTimesCut.value_counts()[:5])\n",
    "print(df.fStrategies.value_counts()[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity checking - making sure we don't need certain columns\n",
    "\n",
    "# ?? is fAllPartsNested always 1 when cNested = cRequired? \n",
    "df_subset = df[df['cNested'] == df['cRequired']]\n",
    "print(df_subset['fAllPartsNested'].unique()) \n",
    "\n",
    "# ?? For Mark: what is the difference between cParts in the Nest table and cNested in the Part table? \n",
    "# diff_df = df[df['cParts'] != df['cNested']]\n",
    "# print(diff_df[['cParts', 'cNested']])\n",
    "\n",
    "# using the training_df, compare the ixAutoNestStrategy and fStrategies columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dataframe of just the features and the target\n",
    "ml_df = df[feature_list + target]\n",
    "\n",
    "# save the DF as a csv for the machine learning model\n",
    "ml_df.to_csv('../CEIP_csv/ml_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the variable of interest\n",
    "# selected_var = 'dCropUtil'\n",
    "\n",
    "# # Compute the correlation coefficients between the selected variable and all other variables\n",
    "# corr_values = df.corr()[selected_var]\n",
    "\n",
    "# # Compute the correlation coefficients between all pairs of variables\n",
    "# corr_values = df.corr()\n",
    "\n",
    "# # Display the correlation coefficients\n",
    "# print(corr_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Display the correlation matrix as a heatmap for better visualization\n",
    "# sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a histogram of the selected variable\n",
    "# sns.histplot(data=df_no_outliers, x=selected_var, kde=True)\n",
    "# plt.title('Histogram of {}'.format(selected_var))\n",
    "# plt.xlabel(selected_var)\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.show()\n",
    "\n",
    "# # Plot a boxplot of the selected variable\n",
    "# sns.boxplot(data=df_no_outliers, x=selected_var)\n",
    "# plt.title('Boxplot of {}'.format(selected_var))\n",
    "# plt.xlabel(selected_var)\n",
    "# plt.show()\n",
    "\n",
    "# # Compute the summary statistics of the selected variable\n",
    "# summary = df_no_outliers[selected_var].describe()\n",
    "# print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove highly correlated features\n",
    "# corr_matrix = df_cleaned.corr().abs()\n",
    "# upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "# cols_to_drop = [column for column in upper_triangle.columns if any(upper_triangle[column] > correlation_threshold)]\n",
    "# df_cleaned.drop(cols_to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the variable of interest\n",
    "# selected_var = 'dCropUtil'\n",
    "\n",
    "# # Compute the correlation coefficients between the selected variable and all other variables\n",
    "# corr_values = df.corr()[selected_var]\n",
    "\n",
    "# # Compute the correlation coefficients between all pairs of variables\n",
    "# corr_values = df.corr()\n",
    "\n",
    "# # Display the correlation coefficients\n",
    "# print(corr_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Display the correlation matrix as a heatmap for better visualization\n",
    "# sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a histogram of the selected variable\n",
    "# sns.histplot(data=df_no_outliers, x=selected_var, kde=True)\n",
    "# plt.title('Histogram of {}'.format(selected_var))\n",
    "# plt.xlabel(selected_var)\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.show()\n",
    "\n",
    "# # Plot a boxplot of the selected variable\n",
    "# sns.boxplot(data=df_no_outliers, x=selected_var)\n",
    "# plt.title('Boxplot of {}'.format(selected_var))\n",
    "# plt.xlabel(selected_var)\n",
    "# plt.show()\n",
    "\n",
    "# # Compute the summary statistics of the selected variable\n",
    "# summary = df_no_outliers[selected_var].describe()\n",
    "# print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove highly correlated features\n",
    "# corr_matrix = df_cleaned.corr().abs()\n",
    "# upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "# cols_to_drop = [column for column in upper_triangle.columns if any(upper_triangle[column] > correlation_threshold)]\n",
    "# df_cleaned.drop(cols_to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variance Inflation Factor (VIF):** VIF is a measure of multicollinearity among features within a multiple regression. It's calculated as the ratio of the variance of the predicted variable to the variance of the model that includes only one feature. If the VIF is equal to 1, there is no multicollinearity among features, but if the VIF exceeds 5 or 10, it's an indication that multicollinearity is high.\n",
    "\n",
    "This code will calculate the VIF for each feature. You may then analyze the \"VIF Factor\" for each feature and consider removing or transforming features that have a high VIF.VIF = 1, Not correlated.\n",
    "1 < VIF < 5, Moderately correlated.\n",
    "VIF >=5, Highly correlated.\n",
    "\n",
    "Most of your features have a VIF near or above 1, which means they are moderately to highly correlated with at least one other feature in your dataset. In particular, 'dLgIntArea' and 'dLgExtConArea' are especially high, almost reaching 1, which indicates a very high degree of correlation.\n",
    "'dPartTrueArea' gives a NaN, which usually indicates a division by zero during the calculation. This could be caused if this feature is a constant or nearly constant value across all observations. You should check this feature for constant values.\n",
    "'dSheetWidth' and 'dSheetLength' have VIF scores less than 1, indicating that they are not significantly correlated with other variables in the dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Index | VIF Factor | Features                 |\n",
    "|-------|------------|--------------------------|\n",
    "| 0     | 0.996952   | dExtArea                 |\n",
    "| 1     | 0.991680   | dSheetArea               |\n",
    "| 2     | 0.967441   | dLgExtConContainedDist   |\n",
    "| 3     | 0.966788   | dExtBoundaryDist         |\n",
    "| 4     | 0.281767   | dSheetWidth              |\n",
    "| 5     | 0.998093   | dLgExtConArea            |\n",
    "| 6     | 0.673872   | dExtContainedDist        |\n",
    "| 7     | 0.956778   | dLgIntBoundaryDist       |\n",
    "| 8     | 0.376538   | dSheetLength             |\n",
    "| 9     | 0.787266   | fExtShape                |\n",
    "| 10    | 0.999993   | dLgIntArea               |\n",
    "| 11    | NaN        | dPartTrueArea            |\n",
    "| 12    | 0.965795   | dLgExtConBoundaryDist    |\n",
    "| 13    | 0.931104   | dLgIntContainedDist      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variance Inflation Factor (VIF)\n",
    "#!! WARNING: This code takes forever to run (like ~5 minutes lol) - commented out \n",
    "# from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# # Get features from the training_df\n",
    "# features = training_df.drop(['calcUtil'], axis=1)\n",
    "\n",
    "# # Calculate VIF\n",
    "# vif = pd.DataFrame()\n",
    "# vif[\"VIF Factor\"] = [variance_inflation_factor(features.values, i) for i in range(features.shape[1])]\n",
    "# vif[\"features\"] = features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vif\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Correlation matrix:** Another way to quantify multicollinearity (a symptom of the curse of dimensionality) is to plot a correlation matrix of your features. Strongly correlated features might be redundant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # interactive heatmap \n",
    "# import plotly.figure_factory as ff\n",
    "\n",
    "# z = corr.values\n",
    "# x = corr.columns.tolist()\n",
    "# y = corr.index.tolist()\n",
    "\n",
    "# heatmap = ff.create_annotated_heatmap(z, x=x, y=y, annotation_text=z.round(2), colorscale='RdBu')\n",
    "# heatmap.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature creation\n",
    "* Making new features based on Mark's - what can we make? See the CEIP_db/Mark_calculations folder for more details on what he did "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a new feature by combining two existing features\n",
    "# df['new_feature'] = df['column1'] * df['column2']\n",
    "\n",
    "# # Create a new feature by applying a mathematical operation\n",
    "# df['log_feature'] = np.log(df['column1'])\n",
    "\n",
    "# # Create a new feature by applying a custom function\n",
    "# def custom_function(x):\n",
    "#     return x**2\n",
    "\n",
    "# df['squared_feature'] = df['column1'].apply(custom_function)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Visualization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Removed Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = px.pie(df, names='fAllPartsNested', title='Distribution of fAllPartsNested')\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# below is bad - figure out better ways to visualize Utilization\n",
    "# fig = px.histogram(df, x='calcUtil')\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cTimesCut - removed, # of times nest will be cut in job, not known in advance \n",
    "# fAllPartsNested - removed, just a 0/1 indicator of if all the parts in the job were nested using automatic nesting\n",
    "# cParts - Total number of parts nested from Nest table - duplicated by cNested, unknown prior to nesting \n",
    "# analysis - check if cParts is identical to cNested, what are the differences \n",
    "# fStrategies - what is the meaning of this? does it add anything? \n",
    "# dNestingTime - analyze later but not a feature \n",
    "# dLengthUsed & dWidthUsed - Length/width of plate used by nested parts - not features \n",
    "# dPartArea - Total area of nested parts - replicated by dPartTrueArea, from Nest Table \n",
    "\n",
    "# visualize dNestingTime vs fStrategies\n",
    "# plt.hist(encoded_df['dNestingTime'], bins=50)\n",
    "# plt.xlabel('dNestingTime')\n",
    "# plt.ylabel('Count')\n",
    "# plt.title('Distribution of dNestingTime')\n",
    "# plt.show()\n",
    "\n",
    "# visualize dNestingTime vs calcUtil "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# automated EDA with AutoViz\n",
    "# eda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'feature' and 'target' with your actual column names\n",
    "# sns.boxplot(x='feature', y='target', data=main_df)\n",
    "# plt.show()\n",
    "\n",
    "# This will create a box plot of the 'target' variable for each value of 'feature',\n",
    "# which can help you understand the relationship between the feature and the target. \n",
    "# You would want to do this for each feature you're considering converting to categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histograms\n",
    "# plt.hist(df_cleaned['column'], bins=20)\n",
    "# plt.xlabel('Column Name')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.title('Histogram of Column Name')\n",
    "# plt.show()\n",
    "\n",
    "# box plots \n",
    "# sns.boxplot(x=df_cleaned['column'])\n",
    "# plt.xlabel('Column Name')\n",
    "# plt.title('Box Plot of Column Name')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plots \n",
    "# plt.scatter(df_cleaned['column1'], df_cleaned['column2'])\n",
    "# plt.xlabel('Column 1')\n",
    "# plt.ylabel('Column 2')\n",
    "# plt.title('Scatter Plot of Column 1 vs. Column 2')\n",
    "# plt.show()\n",
    "\n",
    "# interactive scatter plot \n",
    "# fig = px.scatter(df_cleaned, x='column1', y='column2', color='category_column', hover_data=['column3'])\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bubble plots - 3 numerical variables\n",
    "# plt.scatter(df_cleaned['column1'], df_cleaned['column2'], s=df_cleaned['column3'], alpha=0.5)\n",
    "# plt.xlabel('Column 1')\n",
    "# plt.ylabel('Column 2')\n",
    "# plt.title('Bubble Plot of Column 1 vs. Column 2 (Size = Column 3)')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallel coordinates plot for multiple numerical variables\n",
    "# from pandas.plotting import parallel_coordinates\n",
    "\n",
    "# parallel_coordinates(df_cleaned[['column1', 'column2', 'column3', 'category_column']], 'category_column')\n",
    "# plt.title('Parallel Coordinates Plot')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regression plotting \n",
    "# sns.regplot(x='column1', y='column2', data=df_cleaned)\n",
    "# plt.title('Regression Plot')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Things to visualize: \n",
    "# tradeoffs between nesting time and utilization\n",
    "# distribution of utilization \n",
    "# relationship between fStrategies and utilization\n",
    "# visualization of the frequency of different Strategies used\n",
    "# visualization of the frequency of different Materials used \n",
    "# distribution of Parts, number of Parts per job, number of Parts nested \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.cluster import KMeans\n",
    "\n",
    "# # Create a K-means clustering model with 3 clusters\n",
    "# kmeans = KMeans(n_clusters=3, random_state=0).fit(df)\n",
    "\n",
    "# # Get the cluster labels for each data point\n",
    "# labels = kmeans.labels_\n",
    "\n",
    "# # Add cluster labels to the original DataFrame\n",
    "# df['Cluster'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with PyCaret\n",
    "# import pycaret clustering and init setup\n",
    "# see https://colab.research.google.com/github/pycaret/pycaret/blob/master/tutorials/Tutorial%20-%20Clustering.ipynb#scrollTo=4181de41\n",
    "# from pycaret.clustering import *\n",
    "# s = setup(data, session_id = 123)\n",
    "\n",
    "# # import ClusteringExperiment and init the class\n",
    "# from pycaret.clustering import ClusteringExperiment\n",
    "# exp = ClusteringExperiment()\n",
    "\n",
    "# # init setup on exp\n",
    "# exp.setup(data, session_id = 123)\n",
    "\n",
    "# # train kmeans model\n",
    "# kmeans = create_model('kmeans')\n",
    "\n",
    "# # to check all the available models\n",
    "# models()\n",
    "\n",
    "# # train meanshift model\n",
    "# meanshift = create_model('meanshift')\n",
    "\n",
    "# kmeans_cluster = assign_model(kmeans)\n",
    "# kmeans_cluster\n",
    "\n",
    "# # plot pca cluster plot \n",
    "# plot_model(kmeans, plot = 'cluster')\n",
    "\n",
    "# # plot elbow\n",
    "# plot_model(kmeans, plot = 'elbow')\n",
    "\n",
    "# # plot silhouette\n",
    "# plot_model(kmeans, plot = 'silhouette')\n",
    "\n",
    "# evaluate_model(kmeans)\n",
    "\n",
    "# # predict on test set\n",
    "# kmeans_pred = predict_model(kmeans, data=data)\n",
    "# kmeans_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
