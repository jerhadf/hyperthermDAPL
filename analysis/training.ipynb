{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# can also do a classifer that tries to predict the strategy used given the other columns (part characteristics)\n",
    "\n",
    "# next step: can you make a multi-class classifier + regression model? \n",
    "\n",
    "# classify the strategy -> classify the strategy that yields the optimal util, GIVEN the part characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORE\n",
    "import pandas as pd\n",
    "import numpy as np  # Numpy for numerical computations and array operations\n",
    "import pandas as pd  # Pandas for data manipulation and analysis\n",
    "\n",
    "# MACHINE LEARNING & STATISTICS \n",
    "import scipy.stats as stats  # SciPy for scientific computing and technical computing, including statistics\n",
    "import sklearn as sk # Scikit-learn for machine learning and predictive modeling\n",
    "import pycaret # PyCaret for low-code machine learning and data science automation\n",
    "from pycaret.regression import * # PyCaret's regression module\n",
    "\n",
    "# VISUALIZATION\n",
    "import matplotlib.pyplot as plt  # Matplotlib for creating static, animated, and interactive visualizations\n",
    "import seaborn as sns  # Seaborn for statistical data visualization built on top of Matplotlib\n",
    "import plotly.express as px  # Plotly Express for creating interactive plots and charts\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the ml_data.csv file\n",
    "ml_df = pd.read_csv(\"../CEIP_csv/ml_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "util = 'calcUtil' # define the target"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement XGBoost on this model\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the features and target variables\n",
    "# Assuming that 'target' is the name of the column that we want to predict\n",
    "ml_df.drop('Unnamed: 0', axis=1, inplace=True) # remove unnamed column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPROCESSING FOR XGBOOST \n",
    "# Replace positive infinity with the maximum non-infinity number in the dataset\n",
    "ml_df.replace([np.inf], np.finfo('float32').max, inplace=True)\n",
    "\n",
    "# Replace negative infinity with the minimum non-infinity number in the dataset\n",
    "ml_df.replace([-np.inf], np.finfo('float32').min, inplace=True)\n",
    "\n",
    "# Check if the dataframe still contains NaN and Inf values and replace them with the mean of the column\n",
    "for col in ml_df.columns:\n",
    "    if ml_df[col].isnull().any():\n",
    "        ml_df[col].fillna(ml_df[col].mean(), inplace=True)\n",
    "\n",
    "    if np.isinf(ml_df[col]).any():\n",
    "        ml_df[col].replace([np.inf, -np.inf], ml_df[col].mean(), inplace=True)\n",
    "        \n",
    "#  Replace inf and -inf with np.nan, then drop all rows with np.nan\n",
    "ml_df = ml_df.replace([np.inf, -np.inf], np.nan)\n",
    "ml_df = ml_df.dropna()\n",
    "        \n",
    "ml_df = ml_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove outliers \n",
    "# Compute the IQR\n",
    "Q1 = ml_df.quantile(0.25)\n",
    "Q3 = ml_df.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Determine the outlier cutoff\n",
    "lower_bound = Q1 - 2 * IQR\n",
    "upper_bound = Q3 + 2 * IQR\n",
    "\n",
    "# Create a mask for values within the IQR\n",
    "mask = (ml_df >= lower_bound) & (ml_df <= upper_bound)\n",
    "\n",
    "# Apply the mask to remove outliers\n",
    "ml_df_no_outliers = ml_df[mask].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X = ml_df.drop(util, axis=1)\n",
    "y = ml_df[util]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert everything to the float data type \n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "y_train = y_train.astype('float32')\n",
    "y_test = y_test.astype('float32')\n",
    "\n",
    "# drop all NaN values\n",
    "X_train = X_train.dropna()\n",
    "X_test = X_test.dropna()\n",
    "y_train = y_train.dropna()\n",
    "y_test = y_test.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains infinity or a value too large for dtype('float32').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/jerhadf/Desktop/coding/hyperthermDAPL/analysis/training.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jerhadf/Desktop/coding/hyperthermDAPL/analysis/training.ipynb#X60sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m scaler \u001b[39m=\u001b[39m StandardScaler()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jerhadf/Desktop/coding/hyperthermDAPL/analysis/training.ipynb#X60sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# Fit the scaler and transform the training data\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jerhadf/Desktop/coding/hyperthermDAPL/analysis/training.ipynb#X60sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m X_train_scaled \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39;49mfit_transform(X_train)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jerhadf/Desktop/coding/hyperthermDAPL/analysis/training.ipynb#X60sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# Use the same scaler to transform the test data\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jerhadf/Desktop/coding/hyperthermDAPL/analysis/training.ipynb#X60sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m X_test_scaled \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39mtransform(X_test)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:852\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    848\u001b[0m \u001b[39m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[1;32m    849\u001b[0m \u001b[39m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[1;32m    850\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    851\u001b[0m     \u001b[39m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[0;32m--> 852\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(X, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\u001b[39m.\u001b[39mtransform(X)\n\u001b[1;32m    853\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    854\u001b[0m     \u001b[39m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m    855\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/preprocessing/_data.py:806\u001b[0m, in \u001b[0;36mStandardScaler.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    804\u001b[0m \u001b[39m# Reset internal state before fitting\u001b[39;00m\n\u001b[1;32m    805\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[0;32m--> 806\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpartial_fit(X, y, sample_weight)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/preprocessing/_data.py:841\u001b[0m, in \u001b[0;36mStandardScaler.partial_fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    809\u001b[0m \u001b[39m\"\"\"Online computation of mean and std on X for later scaling.\u001b[39;00m\n\u001b[1;32m    810\u001b[0m \n\u001b[1;32m    811\u001b[0m \u001b[39mAll of X is processed as a single batch. This is intended for cases\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    838\u001b[0m \u001b[39m    Fitted scaler.\u001b[39;00m\n\u001b[1;32m    839\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    840\u001b[0m first_call \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mn_samples_seen_\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 841\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[1;32m    842\u001b[0m     X,\n\u001b[1;32m    843\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcsc\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    844\u001b[0m     estimator\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    845\u001b[0m     dtype\u001b[39m=\u001b[39;49mFLOAT_DTYPES,\n\u001b[1;32m    846\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    847\u001b[0m     reset\u001b[39m=\u001b[39;49mfirst_call,\n\u001b[1;32m    848\u001b[0m )\n\u001b[1;32m    849\u001b[0m n_features \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[1;32m    851\u001b[0m \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:566\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    564\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mValidation should be done on X, y or both.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    565\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 566\u001b[0m     X \u001b[39m=\u001b[39m check_array(X, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[1;32m    567\u001b[0m     out \u001b[39m=\u001b[39m X\n\u001b[1;32m    568\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:800\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    794\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    795\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with dim \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m expected <= 2.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    796\u001b[0m             \u001b[39m%\u001b[39m (array\u001b[39m.\u001b[39mndim, estimator_name)\n\u001b[1;32m    797\u001b[0m         )\n\u001b[1;32m    799\u001b[0m     \u001b[39mif\u001b[39;00m force_all_finite:\n\u001b[0;32m--> 800\u001b[0m         _assert_all_finite(array, allow_nan\u001b[39m=\u001b[39;49mforce_all_finite \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    802\u001b[0m \u001b[39mif\u001b[39;00m ensure_min_samples \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    803\u001b[0m     n_samples \u001b[39m=\u001b[39m _num_samples(array)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:114\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    108\u001b[0m         allow_nan\n\u001b[1;32m    109\u001b[0m         \u001b[39mand\u001b[39;00m np\u001b[39m.\u001b[39misinf(X)\u001b[39m.\u001b[39many()\n\u001b[1;32m    110\u001b[0m         \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m allow_nan\n\u001b[1;32m    111\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39misfinite(X)\u001b[39m.\u001b[39mall()\n\u001b[1;32m    112\u001b[0m     ):\n\u001b[1;32m    113\u001b[0m         type_err \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39minfinity\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m allow_nan \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mNaN, infinity\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 114\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    115\u001b[0m             msg_err\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    116\u001b[0m                 type_err, msg_dtype \u001b[39mif\u001b[39;00m msg_dtype \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m X\u001b[39m.\u001b[39mdtype\n\u001b[1;32m    117\u001b[0m             )\n\u001b[1;32m    118\u001b[0m         )\n\u001b[1;32m    119\u001b[0m \u001b[39m# for object dtype data, we only check for NaNs (GH-13254)\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[39melif\u001b[39;00m X\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m np\u001b[39m.\u001b[39mdtype(\u001b[39m\"\u001b[39m\u001b[39mobject\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m allow_nan:\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains infinity or a value too large for dtype('float32')."
     ]
    }
   ],
   "source": [
    "# apply scaling \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler and transform the training data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Use the same scaler to transform the test data\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "X_train_scaled contains NaN",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/jerhadf/Desktop/coding/hyperthermDAPL/analysis/training.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jerhadf/Desktop/coding/hyperthermDAPL/analysis/training.ipynb#X63sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Check for 'nan' or 'inf' in X_train_scaled and y_train\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jerhadf/Desktop/coding/hyperthermDAPL/analysis/training.ipynb#X63sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39many(np\u001b[39m.\u001b[39misnan(X_train_scaled)), \u001b[39m'\u001b[39m\u001b[39mX_train_scaled contains NaN\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jerhadf/Desktop/coding/hyperthermDAPL/analysis/training.ipynb#X63sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39many(np\u001b[39m.\u001b[39misnan(y_train)), \u001b[39m'\u001b[39m\u001b[39my_train contains NaN\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jerhadf/Desktop/coding/hyperthermDAPL/analysis/training.ipynb#X63sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39many(np\u001b[39m.\u001b[39misinf(X_train_scaled)), \u001b[39m'\u001b[39m\u001b[39mX_train_scaled contains Inf\u001b[39m\u001b[39m'\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: X_train_scaled contains NaN"
     ]
    }
   ],
   "source": [
    "# Check for 'nan' or 'inf' in X_train_scaled and y_train\n",
    "assert not np.any(np.isnan(X_train_scaled)), 'X_train_scaled contains NaN'\n",
    "assert not np.any(np.isnan(y_train)), 'y_train contains NaN'\n",
    "assert not np.any(np.isinf(X_train_scaled)), 'X_train_scaled contains Inf'\n",
    "assert not np.any(np.isinf(y_train)), 'y_train contains Inf'\n",
    "\n",
    "# Check for 'nan' or 'inf' in X_test_scaled and y_test\n",
    "assert not np.any(np.isnan(X_test_scaled)), 'X_test_scaled contains NaN'\n",
    "assert not np.any(np.isnan(y_test)), 'y_test contains NaN'\n",
    "assert not np.any(np.isinf(X_test_scaled)), 'X_test_scaled contains Inf'\n",
    "assert not np.any(np.isinf(y_test)), 'y_test contains Inf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "XGBoostError",
     "evalue": "[19:23:55] /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-10.9-x86_64-cpython-38/xgboost/src/data/data.cc:1104: Check failed: valid: Input data contains `inf` or `nan`\nStack trace:\n  [bt] (0) 1   libxgboost.dylib                    0x00000001354e4785 dmlc::LogMessageFatal::~LogMessageFatal() + 117\n  [bt] (1) 2   libxgboost.dylib                    0x000000013556d4f0 unsigned long long xgboost::SparsePage::Push<xgboost::data::ArrayAdapterBatch>(xgboost::data::ArrayAdapterBatch const&, float, int) + 1280\n  [bt] (2) 3   libxgboost.dylib                    0x00000001355ba71b xgboost::data::SimpleDMatrix::SimpleDMatrix<xgboost::data::ArrayAdapter>(xgboost::data::ArrayAdapter*, float, int) + 299\n  [bt] (3) 4   libxgboost.dylib                    0x000000013556ba35 xgboost::DMatrix* xgboost::DMatrix::Create<xgboost::data::ArrayAdapter>(xgboost::data::ArrayAdapter*, float, int, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>> const&) + 53\n  [bt] (4) 5   libxgboost.dylib                    0x00000001354f6130 XGDMatrixCreateFromDense + 304\n  [bt] (5) 6   libffi.7.dylib                      0x0000000109df0ead ffi_call_unix64 + 85\n  [bt] (6) 7   ???                                 0x000000030303b760 0x0 + 12935477088\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/jerhadf/Desktop/coding/hyperthermDAPL/analysis/training.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jerhadf/Desktop/coding/hyperthermDAPL/analysis/training.ipynb#X46sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m model \u001b[39m=\u001b[39m xgb\u001b[39m.\u001b[39mXGBRegressor(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jerhadf/Desktop/coding/hyperthermDAPL/analysis/training.ipynb#X46sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# train the model \u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jerhadf/Desktop/coding/hyperthermDAPL/analysis/training.ipynb#X46sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[1;32m    619\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[0;32m--> 620\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:988\u001b[0m, in \u001b[0;36mXGBModel.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m    986\u001b[0m \u001b[39mwith\u001b[39;00m config_context(verbosity\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbosity):\n\u001b[1;32m    987\u001b[0m     evals_result: TrainingCallback\u001b[39m.\u001b[39mEvalsLog \u001b[39m=\u001b[39m {}\n\u001b[0;32m--> 988\u001b[0m     train_dmatrix, evals \u001b[39m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[1;32m    989\u001b[0m         missing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmissing,\n\u001b[1;32m    990\u001b[0m         X\u001b[39m=\u001b[39;49mX,\n\u001b[1;32m    991\u001b[0m         y\u001b[39m=\u001b[39;49my,\n\u001b[1;32m    992\u001b[0m         group\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    993\u001b[0m         qid\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    994\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m    995\u001b[0m         base_margin\u001b[39m=\u001b[39;49mbase_margin,\n\u001b[1;32m    996\u001b[0m         feature_weights\u001b[39m=\u001b[39;49mfeature_weights,\n\u001b[1;32m    997\u001b[0m         eval_set\u001b[39m=\u001b[39;49meval_set,\n\u001b[1;32m    998\u001b[0m         sample_weight_eval_set\u001b[39m=\u001b[39;49msample_weight_eval_set,\n\u001b[1;32m    999\u001b[0m         base_margin_eval_set\u001b[39m=\u001b[39;49mbase_margin_eval_set,\n\u001b[1;32m   1000\u001b[0m         eval_group\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m   1001\u001b[0m         eval_qid\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m   1002\u001b[0m         create_dmatrix\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_dmatrix,\n\u001b[1;32m   1003\u001b[0m         enable_categorical\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menable_categorical,\n\u001b[1;32m   1004\u001b[0m         feature_types\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeature_types,\n\u001b[1;32m   1005\u001b[0m     )\n\u001b[1;32m   1006\u001b[0m     params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_xgb_params()\n\u001b[1;32m   1008\u001b[0m     \u001b[39mif\u001b[39;00m callable(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective):\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:448\u001b[0m, in \u001b[0;36m_wrap_evaluation_matrices\u001b[0;34m(missing, X, y, group, qid, sample_weight, base_margin, feature_weights, eval_set, sample_weight_eval_set, base_margin_eval_set, eval_group, eval_qid, create_dmatrix, enable_categorical, feature_types)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_wrap_evaluation_matrices\u001b[39m(\n\u001b[1;32m    429\u001b[0m     missing: \u001b[39mfloat\u001b[39m,\n\u001b[1;32m    430\u001b[0m     X: Any,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    444\u001b[0m     feature_types: Optional[FeatureTypes],\n\u001b[1;32m    445\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Any, List[Tuple[Any, \u001b[39mstr\u001b[39m]]]:\n\u001b[1;32m    446\u001b[0m     \u001b[39m\"\"\"Convert array_like evaluation matrices into DMatrix.  Perform validation on the\u001b[39;00m\n\u001b[1;32m    447\u001b[0m \u001b[39m    way.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 448\u001b[0m     train_dmatrix \u001b[39m=\u001b[39m create_dmatrix(\n\u001b[1;32m    449\u001b[0m         data\u001b[39m=\u001b[39;49mX,\n\u001b[1;32m    450\u001b[0m         label\u001b[39m=\u001b[39;49my,\n\u001b[1;32m    451\u001b[0m         group\u001b[39m=\u001b[39;49mgroup,\n\u001b[1;32m    452\u001b[0m         qid\u001b[39m=\u001b[39;49mqid,\n\u001b[1;32m    453\u001b[0m         weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m    454\u001b[0m         base_margin\u001b[39m=\u001b[39;49mbase_margin,\n\u001b[1;32m    455\u001b[0m         feature_weights\u001b[39m=\u001b[39;49mfeature_weights,\n\u001b[1;32m    456\u001b[0m         missing\u001b[39m=\u001b[39;49mmissing,\n\u001b[1;32m    457\u001b[0m         enable_categorical\u001b[39m=\u001b[39;49menable_categorical,\n\u001b[1;32m    458\u001b[0m         feature_types\u001b[39m=\u001b[39;49mfeature_types,\n\u001b[1;32m    459\u001b[0m         ref\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    460\u001b[0m     )\n\u001b[1;32m    462\u001b[0m     n_validation \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mif\u001b[39;00m eval_set \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mlen\u001b[39m(eval_set)\n\u001b[1;32m    464\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mvalidate_or_none\u001b[39m(meta: Optional[Sequence], name: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Sequence:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:908\u001b[0m, in \u001b[0;36mXGBModel._create_dmatrix\u001b[0;34m(self, ref, **kwargs)\u001b[0m\n\u001b[1;32m    906\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:  \u001b[39m# `QuantileDMatrix` supports lesser types than DMatrix\u001b[39;00m\n\u001b[1;32m    907\u001b[0m         \u001b[39mpass\u001b[39;00m\n\u001b[0;32m--> 908\u001b[0m \u001b[39mreturn\u001b[39;00m DMatrix(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs, nthread\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[1;32m    619\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[0;32m--> 620\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:743\u001b[0m, in \u001b[0;36mDMatrix.__init__\u001b[0;34m(self, data, label, weight, base_margin, missing, silent, feature_names, feature_types, nthread, group, qid, label_lower_bound, label_upper_bound, feature_weights, enable_categorical)\u001b[0m\n\u001b[1;32m    740\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    741\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 743\u001b[0m handle, feature_names, feature_types \u001b[39m=\u001b[39m dispatch_data_backend(\n\u001b[1;32m    744\u001b[0m     data,\n\u001b[1;32m    745\u001b[0m     missing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmissing,\n\u001b[1;32m    746\u001b[0m     threads\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnthread,\n\u001b[1;32m    747\u001b[0m     feature_names\u001b[39m=\u001b[39;49mfeature_names,\n\u001b[1;32m    748\u001b[0m     feature_types\u001b[39m=\u001b[39;49mfeature_types,\n\u001b[1;32m    749\u001b[0m     enable_categorical\u001b[39m=\u001b[39;49menable_categorical,\n\u001b[1;32m    750\u001b[0m )\n\u001b[1;32m    751\u001b[0m \u001b[39massert\u001b[39;00m handle \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    752\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle \u001b[39m=\u001b[39m handle\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/xgboost/data.py:970\u001b[0m, in \u001b[0;36mdispatch_data_backend\u001b[0;34m(data, missing, threads, feature_names, feature_types, enable_categorical)\u001b[0m\n\u001b[1;32m    968\u001b[0m     \u001b[39mreturn\u001b[39;00m _from_tuple(data, missing, threads, feature_names, feature_types)\n\u001b[1;32m    969\u001b[0m \u001b[39mif\u001b[39;00m _is_pandas_df(data):\n\u001b[0;32m--> 970\u001b[0m     \u001b[39mreturn\u001b[39;00m _from_pandas_df(data, enable_categorical, missing, threads,\n\u001b[1;32m    971\u001b[0m                            feature_names, feature_types)\n\u001b[1;32m    972\u001b[0m \u001b[39mif\u001b[39;00m _is_pandas_series(data):\n\u001b[1;32m    973\u001b[0m     \u001b[39mreturn\u001b[39;00m _from_pandas_series(\n\u001b[1;32m    974\u001b[0m         data, missing, threads, enable_categorical, feature_names, feature_types\n\u001b[1;32m    975\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/xgboost/data.py:420\u001b[0m, in \u001b[0;36m_from_pandas_df\u001b[0;34m(data, enable_categorical, missing, nthread, feature_names, feature_types)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_from_pandas_df\u001b[39m(\n\u001b[1;32m    410\u001b[0m     data: DataFrame,\n\u001b[1;32m    411\u001b[0m     enable_categorical: \u001b[39mbool\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    415\u001b[0m     feature_types: Optional[FeatureTypes],\n\u001b[1;32m    416\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DispatchedDataBackendReturnType:\n\u001b[1;32m    417\u001b[0m     data, feature_names, feature_types \u001b[39m=\u001b[39m _transform_pandas_df(\n\u001b[1;32m    418\u001b[0m         data, enable_categorical, feature_names, feature_types\n\u001b[1;32m    419\u001b[0m     )\n\u001b[0;32m--> 420\u001b[0m     \u001b[39mreturn\u001b[39;00m _from_numpy_array(data, missing, nthread, feature_names, feature_types)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/xgboost/data.py:213\u001b[0m, in \u001b[0;36m_from_numpy_array\u001b[0;34m(data, missing, nthread, feature_names, feature_types)\u001b[0m\n\u001b[1;32m    208\u001b[0m args \u001b[39m=\u001b[39m {\n\u001b[1;32m    209\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mmissing\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mfloat\u001b[39m(missing),\n\u001b[1;32m    210\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mnthread\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mint\u001b[39m(nthread),\n\u001b[1;32m    211\u001b[0m }\n\u001b[1;32m    212\u001b[0m config \u001b[39m=\u001b[39m \u001b[39mbytes\u001b[39m(json\u001b[39m.\u001b[39mdumps(args), \u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 213\u001b[0m _check_call(\n\u001b[1;32m    214\u001b[0m     _LIB\u001b[39m.\u001b[39;49mXGDMatrixCreateFromDense(\n\u001b[1;32m    215\u001b[0m         _array_interface(data),\n\u001b[1;32m    216\u001b[0m         config,\n\u001b[1;32m    217\u001b[0m         ctypes\u001b[39m.\u001b[39;49mbyref(handle),\n\u001b[1;32m    218\u001b[0m     )\n\u001b[1;32m    219\u001b[0m )\n\u001b[1;32m    220\u001b[0m \u001b[39mreturn\u001b[39;00m handle, feature_names, feature_types\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:279\u001b[0m, in \u001b[0;36m_check_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[39m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \n\u001b[1;32m    270\u001b[0m \u001b[39mThis function will raise exception when error occurs.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[39m    return value from API calls\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 279\u001b[0m     \u001b[39mraise\u001b[39;00m XGBoostError(py_str(_LIB\u001b[39m.\u001b[39mXGBGetLastError()))\n",
      "\u001b[0;31mXGBoostError\u001b[0m: [19:23:55] /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-10.9-x86_64-cpython-38/xgboost/src/data/data.cc:1104: Check failed: valid: Input data contains `inf` or `nan`\nStack trace:\n  [bt] (0) 1   libxgboost.dylib                    0x00000001354e4785 dmlc::LogMessageFatal::~LogMessageFatal() + 117\n  [bt] (1) 2   libxgboost.dylib                    0x000000013556d4f0 unsigned long long xgboost::SparsePage::Push<xgboost::data::ArrayAdapterBatch>(xgboost::data::ArrayAdapterBatch const&, float, int) + 1280\n  [bt] (2) 3   libxgboost.dylib                    0x00000001355ba71b xgboost::data::SimpleDMatrix::SimpleDMatrix<xgboost::data::ArrayAdapter>(xgboost::data::ArrayAdapter*, float, int) + 299\n  [bt] (3) 4   libxgboost.dylib                    0x000000013556ba35 xgboost::DMatrix* xgboost::DMatrix::Create<xgboost::data::ArrayAdapter>(xgboost::data::ArrayAdapter*, float, int, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>> const&) + 53\n  [bt] (4) 5   libxgboost.dylib                    0x00000001354f6130 XGDMatrixCreateFromDense + 304\n  [bt] (5) 6   libffi.7.dylib                      0x0000000109df0ead ffi_call_unix64 + 85\n  [bt] (6) 7   ???                                 0x000000030303b760 0x0 + 12935477088\n\n"
     ]
    }
   ],
   "source": [
    "# Create an XGBoost model\n",
    "# use the regression model to minimize mean squared error in predictions \n",
    "\n",
    "# set model parameters \n",
    "params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'colsample_bytree': 0.5,\n",
    "    'learning_rate': 0.1,\n",
    "    'max_depth': 5,\n",
    "    'alpha': 10,\n",
    "    'n_estimators': 100,\n",
    "    'subsample': 0.8,\n",
    "    'min_child_weight': 1,\n",
    "    'gamma': 0\n",
    "}\n",
    "\n",
    "model = xgb.XGBRegressor(**params)\n",
    "\n",
    "# train the model \n",
    "model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take the first 100,000 rows as an example\n",
    "small_X_train = X_train[:100000]\n",
    "small_y_train = y_train[:100000]\n",
    "\n",
    "# Fit the model on the smaller subset\n",
    "model.fit(small_X_train, small_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic models to try \n",
    "* Linear Regression - Use sci-kit learn to implement\n",
    "* Logistic Regression\n",
    "* Support Vector Machines\n",
    "* Basic decision trees\n",
    "* Naive Bayes\n",
    "\n",
    "### XGBoost \n",
    "Tree ensemble model that can handle tabular, numerical, low-dimensional data very well. Fast and scalable, and can be hyperparameter tuned to optimize performance.\n",
    "\n",
    "### ResNet-like architecture\n",
    "Neural network architecture adapted for tabular data, allowing learning from shallow and deep features. Use PyTorch or TensorFlow to implement. Less interpretable. \n",
    "\n",
    "### Transformer \n",
    "Use Transformers with tabular data? \n",
    "\n",
    "### KNN - K Nearest Neighbors\n",
    "Predicts the target variable based on the similarity of the features with the nearest neighbors in the training dataset. Computationally expensive, but can be used for regression and classification."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "* Cross-validation\n",
    "* Classification Accuracy\n",
    "* Confusion Matrix\n",
    "* ROC-AUC Curve\n",
    "  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Fine-tuning\n",
    "* Hyperparameter tuning\n",
    "* Ensembling multiple models together - voting classifier, bagging, boosting, XGBoost "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Univariate Feature Selection:** This method uses statistical tests like chi-squared tests or ANOVA to evaluate the relationship between each feature and the target variable independently. It ranks the features based on their significance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_selection import SelectKBest, chi2, f_classif\n",
    "\n",
    "# # For categorical target using Chi-squared test\n",
    "# selector = SelectKBest(score_func=chi2, k=5)\n",
    "# selector.fit_transform(X, y)\n",
    "\n",
    "# # For continuous target using ANOVA F-value\n",
    "# selector = SelectKBest(score_func=f_classif, k=5)\n",
    "# selector.fit_transform(X, y)\n",
    "\n",
    "# # Get feature importances\n",
    "# scores = selector.scores_\n",
    "# feature_importances = pd.DataFrame({'feature': X.columns, 'importance': scores})\n",
    "# feature_importances = feature_importances.sort_values('importance', ascending=False)\n",
    "# print(feature_importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other statistical testing \n",
    "# import scipy.stats as stats\n",
    "\n",
    "# # Perform a t-test to compare the means of two groups\n",
    "# t_stat, p_value = stats.ttest_ind(group1, group2)\n",
    "# print(\"T-statistic:\", t_stat, \"P-value:\", p_value)\n",
    "\n",
    "# # Perform a chi-squared test to determine the association between two categorical variables\n",
    "# chi_stat, p_value, dof, ex = stats.chi2_contingency(contingency_table)\n",
    "# print(\"Chi-squared statistic:\", chi_stat, \"P-value:\", p_value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
