{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# can also do a classifer that tries to predict the strategy used given the other columns (part characteristics)\n",
    "\n",
    "# next step: can you make a multi-class classifier + regression model? \n",
    "\n",
    "# classify the strategy -> classify the strategy that yields the optimal util, GIVEN the part characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORE\n",
    "import pandas as pd\n",
    "import numpy as np  # Numpy for numerical computations and array operations\n",
    "import pandas as pd  # Pandas for data manipulation and analysis\n",
    "\n",
    "# MACHINE LEARNING & STATISTICS \n",
    "import scipy.stats as stats  # SciPy for scientific computing and technical computing, including statistics\n",
    "import sklearn as sk # Scikit-learn for machine learning and predictive modeling\n",
    "import pycaret # PyCaret for low-code machine learning and data science automation\n",
    "from pycaret.regression import * # PyCaret's regression module\n",
    "\n",
    "# VISUALIZATION\n",
    "import matplotlib.pyplot as plt  # Matplotlib for creating static, animated, and interactive visualizations\n",
    "import seaborn as sns  # Seaborn for statistical data visualization built on top of Matplotlib\n",
    "import plotly.express as px  # Plotly Express for creating interactive plots and charts\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the ml_data.csv file\n",
    "filepath = \"../CEIP_csv/ml_data_nooutliers.csv\"\n",
    "ml_df = pd.read_csv(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "util = 'calcUtil' # define the target"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement XGBoost on this model\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the features and target variables\n",
    "# Assuming that 'target' is the name of the column that we want to predict\n",
    "# ml_df.drop('Unnamed: 0', axis=1, inplace=True) # remove unnamed column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPROCESSING FOR XGBOOST \n",
    "# Replace positive infinity with the maximum non-infinity number in the dataset\n",
    "ml_df.replace([np.inf], np.finfo('float32').max, inplace=True)\n",
    "\n",
    "# Replace negative infinity with the minimum non-infinity number in the dataset\n",
    "ml_df.replace([-np.inf], np.finfo('float32').min, inplace=True)\n",
    "\n",
    "# Check if the dataframe still contains NaN and Inf values and replace them with the mean of the column\n",
    "for col in ml_df.columns:\n",
    "    if ml_df[col].isnull().any():\n",
    "        ml_df[col].fillna(ml_df[col].mean(), inplace=True)\n",
    "\n",
    "    if np.isinf(ml_df[col]).any():\n",
    "        ml_df[col].replace([np.inf, -np.inf], ml_df[col].mean(), inplace=True)\n",
    "        \n",
    "#  Replace inf and -inf with np.nan, then drop all rows with np.nan\n",
    "ml_df = ml_df.replace([np.inf, -np.inf], np.nan)\n",
    "ml_df = ml_df.dropna()\n",
    "        \n",
    "ml_df = ml_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove outliers \n",
    "# Compute the IQR\n",
    "# Q1 = ml_df.quantile(0.25)\n",
    "# Q3 = ml_df.quantile(0.75)\n",
    "# IQR = Q3 - Q1\n",
    "\n",
    "# # Determine the outlier cutoff\n",
    "# lower_bound = Q1 - 2 * IQR\n",
    "# upper_bound = Q3 + 2 * IQR\n",
    "\n",
    "# # Create a mask for values within the IQR\n",
    "# mask = (ml_df >= lower_bound) & (ml_df <= upper_bound)\n",
    "\n",
    "# # Apply the mask to remove outliers\n",
    "# ml_df_no_outliers = ml_df[mask].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X = ml_df.drop(util, axis=1)\n",
    "y = ml_df[util]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert everything to the float data type \n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "y_train = y_train.astype('float32')\n",
    "y_test = y_test.astype('float32')\n",
    "\n",
    "# drop all NaN values\n",
    "X_train = X_train.dropna()\n",
    "X_test = X_test.dropna()\n",
    "y_train = y_train.dropna()\n",
    "y_test = y_test.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply scaling \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler and transform the training data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Use the same scaler to transform the test data\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "X_train_scaled contains NaN",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/jerhadf/Desktop/coding/hyperthermDAPL/analysis/training.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jerhadf/Desktop/coding/hyperthermDAPL/analysis/training.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Check for 'nan' or 'inf' in X_train_scaled and y_train\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jerhadf/Desktop/coding/hyperthermDAPL/analysis/training.ipynb#X16sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39many(np\u001b[39m.\u001b[39misnan(X_train_scaled)), \u001b[39m'\u001b[39m\u001b[39mX_train_scaled contains NaN\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jerhadf/Desktop/coding/hyperthermDAPL/analysis/training.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39many(np\u001b[39m.\u001b[39misnan(y_train)), \u001b[39m'\u001b[39m\u001b[39my_train contains NaN\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jerhadf/Desktop/coding/hyperthermDAPL/analysis/training.ipynb#X16sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39many(np\u001b[39m.\u001b[39misinf(X_train_scaled)), \u001b[39m'\u001b[39m\u001b[39mX_train_scaled contains Inf\u001b[39m\u001b[39m'\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: X_train_scaled contains NaN"
     ]
    }
   ],
   "source": [
    "# Check for 'nan' or 'inf' in X_train_scaled and y_train\n",
    "assert not np.any(np.isnan(X_train_scaled)), 'X_train_scaled contains NaN'\n",
    "assert not np.any(np.isnan(y_train)), 'y_train contains NaN'\n",
    "assert not np.any(np.isinf(X_train_scaled)), 'X_train_scaled contains Inf'\n",
    "assert not np.any(np.isinf(y_train)), 'y_train contains Inf'\n",
    "\n",
    "# Check for 'nan' or 'inf' in X_test_scaled and y_test\n",
    "assert not np.any(np.isnan(X_test_scaled)), 'X_test_scaled contains NaN'\n",
    "assert not np.any(np.isnan(y_test)), 'y_test contains NaN'\n",
    "assert not np.any(np.isinf(X_test_scaled)), 'X_test_scaled contains Inf'\n",
    "assert not np.any(np.isinf(y_test)), 'y_test contains Inf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "XGBoostError",
     "evalue": "[21:23:52] /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-10.9-x86_64-cpython-38/xgboost/src/data/data.cc:461: Check failed: valid: Label contains NaN, infinity or a value too large.\nStack trace:\n  [bt] (0) 1   libxgboost.dylib                    0x0000000139862785 dmlc::LogMessageFatal::~LogMessageFatal() + 117\n  [bt] (1) 2   libxgboost.dylib                    0x00000001398f8549 xgboost::MetaInfo::SetInfoFromHost(xgboost::GenericParameter const&, xgboost::StringView, xgboost::Json) + 4585\n  [bt] (2) 3   libxgboost.dylib                    0x00000001398f7272 xgboost::MetaInfo::SetInfo(xgboost::GenericParameter const&, xgboost::StringView, xgboost::StringView) + 146\n  [bt] (3) 4   libxgboost.dylib                    0x0000000139878e69 XGDMatrixSetInfoFromInterface + 233\n  [bt] (4) 5   libffi.7.dylib                      0x000000010e15dead ffi_call_unix64 + 85\n  [bt] (5) 6   ???                                 0x000000030b510e40 0x0 + 13074763328\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/jerhadf/Desktop/coding/hyperthermDAPL/analysis/training.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jerhadf/Desktop/coding/hyperthermDAPL/analysis/training.ipynb#X20sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m model \u001b[39m=\u001b[39m xgb\u001b[39m.\u001b[39mXGBRegressor(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jerhadf/Desktop/coding/hyperthermDAPL/analysis/training.ipynb#X20sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# train the model \u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jerhadf/Desktop/coding/hyperthermDAPL/analysis/training.ipynb#X20sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_train_scaled, y_train)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[1;32m    619\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[0;32m--> 620\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:988\u001b[0m, in \u001b[0;36mXGBModel.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m    986\u001b[0m \u001b[39mwith\u001b[39;00m config_context(verbosity\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbosity):\n\u001b[1;32m    987\u001b[0m     evals_result: TrainingCallback\u001b[39m.\u001b[39mEvalsLog \u001b[39m=\u001b[39m {}\n\u001b[0;32m--> 988\u001b[0m     train_dmatrix, evals \u001b[39m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[1;32m    989\u001b[0m         missing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmissing,\n\u001b[1;32m    990\u001b[0m         X\u001b[39m=\u001b[39;49mX,\n\u001b[1;32m    991\u001b[0m         y\u001b[39m=\u001b[39;49my,\n\u001b[1;32m    992\u001b[0m         group\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    993\u001b[0m         qid\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    994\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m    995\u001b[0m         base_margin\u001b[39m=\u001b[39;49mbase_margin,\n\u001b[1;32m    996\u001b[0m         feature_weights\u001b[39m=\u001b[39;49mfeature_weights,\n\u001b[1;32m    997\u001b[0m         eval_set\u001b[39m=\u001b[39;49meval_set,\n\u001b[1;32m    998\u001b[0m         sample_weight_eval_set\u001b[39m=\u001b[39;49msample_weight_eval_set,\n\u001b[1;32m    999\u001b[0m         base_margin_eval_set\u001b[39m=\u001b[39;49mbase_margin_eval_set,\n\u001b[1;32m   1000\u001b[0m         eval_group\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m   1001\u001b[0m         eval_qid\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m   1002\u001b[0m         create_dmatrix\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_dmatrix,\n\u001b[1;32m   1003\u001b[0m         enable_categorical\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menable_categorical,\n\u001b[1;32m   1004\u001b[0m         feature_types\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeature_types,\n\u001b[1;32m   1005\u001b[0m     )\n\u001b[1;32m   1006\u001b[0m     params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_xgb_params()\n\u001b[1;32m   1008\u001b[0m     \u001b[39mif\u001b[39;00m callable(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective):\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:448\u001b[0m, in \u001b[0;36m_wrap_evaluation_matrices\u001b[0;34m(missing, X, y, group, qid, sample_weight, base_margin, feature_weights, eval_set, sample_weight_eval_set, base_margin_eval_set, eval_group, eval_qid, create_dmatrix, enable_categorical, feature_types)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_wrap_evaluation_matrices\u001b[39m(\n\u001b[1;32m    429\u001b[0m     missing: \u001b[39mfloat\u001b[39m,\n\u001b[1;32m    430\u001b[0m     X: Any,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    444\u001b[0m     feature_types: Optional[FeatureTypes],\n\u001b[1;32m    445\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Any, List[Tuple[Any, \u001b[39mstr\u001b[39m]]]:\n\u001b[1;32m    446\u001b[0m     \u001b[39m\"\"\"Convert array_like evaluation matrices into DMatrix.  Perform validation on the\u001b[39;00m\n\u001b[1;32m    447\u001b[0m \u001b[39m    way.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 448\u001b[0m     train_dmatrix \u001b[39m=\u001b[39m create_dmatrix(\n\u001b[1;32m    449\u001b[0m         data\u001b[39m=\u001b[39;49mX,\n\u001b[1;32m    450\u001b[0m         label\u001b[39m=\u001b[39;49my,\n\u001b[1;32m    451\u001b[0m         group\u001b[39m=\u001b[39;49mgroup,\n\u001b[1;32m    452\u001b[0m         qid\u001b[39m=\u001b[39;49mqid,\n\u001b[1;32m    453\u001b[0m         weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m    454\u001b[0m         base_margin\u001b[39m=\u001b[39;49mbase_margin,\n\u001b[1;32m    455\u001b[0m         feature_weights\u001b[39m=\u001b[39;49mfeature_weights,\n\u001b[1;32m    456\u001b[0m         missing\u001b[39m=\u001b[39;49mmissing,\n\u001b[1;32m    457\u001b[0m         enable_categorical\u001b[39m=\u001b[39;49menable_categorical,\n\u001b[1;32m    458\u001b[0m         feature_types\u001b[39m=\u001b[39;49mfeature_types,\n\u001b[1;32m    459\u001b[0m         ref\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    460\u001b[0m     )\n\u001b[1;32m    462\u001b[0m     n_validation \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mif\u001b[39;00m eval_set \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mlen\u001b[39m(eval_set)\n\u001b[1;32m    464\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mvalidate_or_none\u001b[39m(meta: Optional[Sequence], name: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Sequence:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:908\u001b[0m, in \u001b[0;36mXGBModel._create_dmatrix\u001b[0;34m(self, ref, **kwargs)\u001b[0m\n\u001b[1;32m    906\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:  \u001b[39m# `QuantileDMatrix` supports lesser types than DMatrix\u001b[39;00m\n\u001b[1;32m    907\u001b[0m         \u001b[39mpass\u001b[39;00m\n\u001b[0;32m--> 908\u001b[0m \u001b[39mreturn\u001b[39;00m DMatrix(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs, nthread\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[1;32m    619\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[0;32m--> 620\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:754\u001b[0m, in \u001b[0;36mDMatrix.__init__\u001b[0;34m(self, data, label, weight, base_margin, missing, silent, feature_names, feature_types, nthread, group, qid, label_lower_bound, label_upper_bound, feature_weights, enable_categorical)\u001b[0m\n\u001b[1;32m    751\u001b[0m \u001b[39massert\u001b[39;00m handle \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    752\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle \u001b[39m=\u001b[39m handle\n\u001b[0;32m--> 754\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mset_info(\n\u001b[1;32m    755\u001b[0m     label\u001b[39m=\u001b[39;49mlabel,\n\u001b[1;32m    756\u001b[0m     weight\u001b[39m=\u001b[39;49mweight,\n\u001b[1;32m    757\u001b[0m     base_margin\u001b[39m=\u001b[39;49mbase_margin,\n\u001b[1;32m    758\u001b[0m     group\u001b[39m=\u001b[39;49mgroup,\n\u001b[1;32m    759\u001b[0m     qid\u001b[39m=\u001b[39;49mqid,\n\u001b[1;32m    760\u001b[0m     label_lower_bound\u001b[39m=\u001b[39;49mlabel_lower_bound,\n\u001b[1;32m    761\u001b[0m     label_upper_bound\u001b[39m=\u001b[39;49mlabel_upper_bound,\n\u001b[1;32m    762\u001b[0m     feature_weights\u001b[39m=\u001b[39;49mfeature_weights,\n\u001b[1;32m    763\u001b[0m )\n\u001b[1;32m    765\u001b[0m \u001b[39mif\u001b[39;00m feature_names \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    766\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_names \u001b[39m=\u001b[39m feature_names\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[1;32m    619\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[0;32m--> 620\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:819\u001b[0m, in \u001b[0;36mDMatrix.set_info\u001b[0;34m(self, label, weight, base_margin, group, qid, label_lower_bound, label_upper_bound, feature_names, feature_types, feature_weights)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m dispatch_meta_backend\n\u001b[1;32m    818\u001b[0m \u001b[39mif\u001b[39;00m label \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 819\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mset_label(label)\n\u001b[1;32m    820\u001b[0m \u001b[39mif\u001b[39;00m weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    821\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_weight(weight)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:950\u001b[0m, in \u001b[0;36mDMatrix.set_label\u001b[0;34m(self, label)\u001b[0m\n\u001b[1;32m    942\u001b[0m \u001b[39m\"\"\"Set label of dmatrix\u001b[39;00m\n\u001b[1;32m    943\u001b[0m \n\u001b[1;32m    944\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[39m    The label information to be set into DMatrix\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    949\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m dispatch_meta_backend\n\u001b[0;32m--> 950\u001b[0m dispatch_meta_backend(\u001b[39mself\u001b[39;49m, label, \u001b[39m'\u001b[39;49m\u001b[39mlabel\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mfloat\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/xgboost/data.py:1134\u001b[0m, in \u001b[0;36mdispatch_meta_backend\u001b[0;34m(matrix, data, name, dtype)\u001b[0m\n\u001b[1;32m   1132\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m \u001b[39mif\u001b[39;00m _is_pandas_series(data):\n\u001b[0;32m-> 1134\u001b[0m     _meta_from_pandas_series(data, name, dtype, handle)\n\u001b[1;32m   1135\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m \u001b[39mif\u001b[39;00m _is_dlpack(data):\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/xgboost/data.py:443\u001b[0m, in \u001b[0;36m_meta_from_pandas_series\u001b[0;34m(data, name, dtype, handle)\u001b[0m\n\u001b[1;32m    441\u001b[0m     data \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mto_dense()  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m    442\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(data\u001b[39m.\u001b[39mshape) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mor\u001b[39;00m data\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m data\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 443\u001b[0m _meta_from_numpy(data, name, dtype, handle)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/xgboost/data.py:1050\u001b[0m, in \u001b[0;36m_meta_from_numpy\u001b[0;34m(data, field, dtype, handle)\u001b[0m\n\u001b[1;32m   1048\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mMasked array is not supported.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1049\u001b[0m interface_str \u001b[39m=\u001b[39m _array_interface(data)\n\u001b[0;32m-> 1050\u001b[0m _check_call(_LIB\u001b[39m.\u001b[39;49mXGDMatrixSetInfoFromInterface(handle, c_str(field), interface_str))\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:279\u001b[0m, in \u001b[0;36m_check_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[39m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \n\u001b[1;32m    270\u001b[0m \u001b[39mThis function will raise exception when error occurs.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[39m    return value from API calls\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 279\u001b[0m     \u001b[39mraise\u001b[39;00m XGBoostError(py_str(_LIB\u001b[39m.\u001b[39mXGBGetLastError()))\n",
      "\u001b[0;31mXGBoostError\u001b[0m: [21:23:52] /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-10.9-x86_64-cpython-38/xgboost/src/data/data.cc:461: Check failed: valid: Label contains NaN, infinity or a value too large.\nStack trace:\n  [bt] (0) 1   libxgboost.dylib                    0x0000000139862785 dmlc::LogMessageFatal::~LogMessageFatal() + 117\n  [bt] (1) 2   libxgboost.dylib                    0x00000001398f8549 xgboost::MetaInfo::SetInfoFromHost(xgboost::GenericParameter const&, xgboost::StringView, xgboost::Json) + 4585\n  [bt] (2) 3   libxgboost.dylib                    0x00000001398f7272 xgboost::MetaInfo::SetInfo(xgboost::GenericParameter const&, xgboost::StringView, xgboost::StringView) + 146\n  [bt] (3) 4   libxgboost.dylib                    0x0000000139878e69 XGDMatrixSetInfoFromInterface + 233\n  [bt] (4) 5   libffi.7.dylib                      0x000000010e15dead ffi_call_unix64 + 85\n  [bt] (5) 6   ???                                 0x000000030b510e40 0x0 + 13074763328\n\n"
     ]
    }
   ],
   "source": [
    "# Create an XGBoost model\n",
    "# use the regression model to minimize mean squared error in predictions \n",
    "\n",
    "# set model parameters \n",
    "params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'colsample_bytree': 0.5,\n",
    "    'learning_rate': 0.1,\n",
    "    'max_depth': 5,\n",
    "    'alpha': 10,\n",
    "    'n_estimators': 100,\n",
    "    'subsample': 0.8,\n",
    "    'min_child_weight': 1,\n",
    "    'gamma': 0\n",
    "}\n",
    "\n",
    "model = xgb.XGBRegressor(**params)\n",
    "\n",
    "# train the model \n",
    "model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take the first 100,000 rows as an example\n",
    "small_X_train = X_train[:100000]\n",
    "small_y_train = y_train[:100000]\n",
    "\n",
    "# Fit the model on the smaller subset\n",
    "model.fit(small_X_train, small_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic models to try \n",
    "* Linear Regression - Use sci-kit learn to implement\n",
    "* Logistic Regression\n",
    "* Support Vector Machines\n",
    "* Basic decision trees\n",
    "* Naive Bayes\n",
    "\n",
    "### XGBoost \n",
    "Tree ensemble model that can handle tabular, numerical, low-dimensional data very well. Fast and scalable, and can be hyperparameter tuned to optimize performance.\n",
    "\n",
    "### ResNet-like architecture\n",
    "Neural network architecture adapted for tabular data, allowing learning from shallow and deep features. Use PyTorch or TensorFlow to implement. Less interpretable. \n",
    "\n",
    "### Transformer \n",
    "Use Transformers with tabular data? \n",
    "\n",
    "### KNN - K Nearest Neighbors\n",
    "Predicts the target variable based on the similarity of the features with the nearest neighbors in the training dataset. Computationally expensive, but can be used for regression and classification."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "* Cross-validation\n",
    "* Classification Accuracy\n",
    "* Confusion Matrix\n",
    "* ROC-AUC Curve\n",
    "  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Fine-tuning\n",
    "* Hyperparameter tuning\n",
    "* Ensembling multiple models together - voting classifier, bagging, boosting, XGBoost "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Univariate Feature Selection:** This method uses statistical tests like chi-squared tests or ANOVA to evaluate the relationship between each feature and the target variable independently. It ranks the features based on their significance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_selection import SelectKBest, chi2, f_classif\n",
    "\n",
    "# # For categorical target using Chi-squared test\n",
    "# selector = SelectKBest(score_func=chi2, k=5)\n",
    "# selector.fit_transform(X, y)\n",
    "\n",
    "# # For continuous target using ANOVA F-value\n",
    "# selector = SelectKBest(score_func=f_classif, k=5)\n",
    "# selector.fit_transform(X, y)\n",
    "\n",
    "# # Get feature importances\n",
    "# scores = selector.scores_\n",
    "# feature_importances = pd.DataFrame({'feature': X.columns, 'importance': scores})\n",
    "# feature_importances = feature_importances.sort_values('importance', ascending=False)\n",
    "# print(feature_importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other statistical testing \n",
    "# import scipy.stats as stats\n",
    "\n",
    "# # Perform a t-test to compare the means of two groups\n",
    "# t_stat, p_value = stats.ttest_ind(group1, group2)\n",
    "# print(\"T-statistic:\", t_stat, \"P-value:\", p_value)\n",
    "\n",
    "# # Perform a chi-squared test to determine the association between two categorical variables\n",
    "# chi_stat, p_value, dof, ex = stats.chi2_contingency(contingency_table)\n",
    "# print(\"Chi-squared statistic:\", chi_stat, \"P-value:\", p_value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
